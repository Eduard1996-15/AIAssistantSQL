# üöÄ Optimizaci√≥n de GPU para Ollama

## Tu Hardware
- **GPU:** NVIDIA RTX 4070 ‚úÖ (Excelente para IA)
- **RAM:** 64GB DDR5 ‚úÖ (M√°s que suficiente)
- **CPU:** Intel i9-13900HX ‚úÖ (Potente)

## Estado Actual
```
deepseek-coder:33b    67% CPU / 33% GPU
```

**Problema:** Ollama est√° usando m√°s CPU que GPU (deber√≠a ser al rev√©s).

---

## ‚úÖ PASO 1: Configurar Variables de Entorno

### Opci√≥n A: Configuraci√≥n Permanente (Recomendado)

1. **Abrir Variables de Entorno:**
   - Presiona `Win + X` ‚Üí "Sistema" ‚Üí "Configuraci√≥n avanzada del sistema"
   - Click en "Variables de entorno"

2. **Crear nuevas variables (Sistema):**

```
Variable: OLLAMA_NUM_GPU
Valor: 1

Variable: OLLAMA_GPU_OVERHEAD
Valor: 0

Variable: OLLAMA_MAX_LOADED_MODELS
Valor: 1

Variable: OLLAMA_NUM_PARALLEL
Valor: 1

Variable: CUDA_VISIBLE_DEVICES
Valor: 0
```

3. **Reiniciar Ollama:**
```powershell
# Cerrar Ollama completamente
taskkill /F /IM ollama.exe

# Iniciar de nuevo
ollama serve
```

### Opci√≥n B: Configuraci√≥n Temporal (Para probar)

```powershell
# En PowerShell ANTES de iniciar Ollama:
$env:OLLAMA_NUM_GPU = "1"
$env:OLLAMA_GPU_OVERHEAD = "0"
$env:OLLAMA_MAX_LOADED_MODELS = "1"
$env:OLLAMA_NUM_PARALLEL = "1"
$env:CUDA_VISIBLE_DEVICES = "0"

# Luego iniciar Ollama
ollama serve
```

---

## ‚úÖ PASO 2: Verificar CUDA

```powershell
# Verificar que NVIDIA drivers est√°n instalados
nvidia-smi
```

**Deber√≠as ver:**
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 535.xx       Driver Version: 535.xx       CUDA Version: 12.x    |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |
| N/A   45C    P8    10W /  N/A |      0MiB / 12288MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
```

---

## ‚úÖ PASO 3: Ejecutar con M√°s GPU

Despu√©s de configurar variables de entorno:

```powershell
# Cerrar todo
taskkill /F /IM ollama.exe

# Iniciar Ollama con configuraci√≥n GPU
ollama serve

# En otra terminal, cargar el modelo
ollama run deepseek-coder:33b

# Verificar uso de GPU
ollama ps
```

**Resultado esperado:**
```
NAME                  ID              SIZE     PROCESSOR          CONTEXT    UNTIL
deepseek-coder:33b    acec7c0b0fd9    21 GB    10%/90% CPU/GPU    4096       Now
                                               ^^^^^^^ M√ÅS GPU!
```

---

## üéØ Rendimiento Esperado

| Configuraci√≥n | Tiempo Consulta Simple | Tiempo Consulta Compleja |
|---------------|------------------------|--------------------------|
| **Solo CPU** (actual) | 45-90s | 90-180s |
| **CPU + GPU (33%)** | 30-60s | 60-120s |
| **CPU + GPU (90%)** | **10-25s** ‚ö° | **25-60s** ‚ö° |

---

## üîß Troubleshooting

### GPU no se detecta

```powershell
# 1. Verificar drivers NVIDIA
nvidia-smi

# 2. Reinstalar Ollama (asegura que detecte GPU)
# Descargar desde: https://ollama.ai/download

# 3. Verificar versi√≥n
ollama --version
```

### Sigue usando mucho CPU

```powershell
# Forzar solo GPU (experimental)
$env:OLLAMA_NUM_GPU = "1"
$env:OLLAMA_GPU_LAYERS = "999"  # Forzar TODAS las capas a GPU

ollama serve
```

---

## üìä Monitoreo en Tiempo Real

### Ver uso de GPU mientras procesas consultas:

```powershell
# Terminal 1: Ollama
ollama serve

# Terminal 2: Monitoreo GPU (actualiza cada 1 segundo)
nvidia-smi -l 1

# Terminal 3: Tu aplicaci√≥n
cd C:\Users\Admin\Documents\PROYECTOS DGAT\AIAssistantSQL\AIAssistantSQL
dotnet run
```

---

## ‚úÖ Configuraci√≥n √ìptima para RTX 4070

```powershell
# Variables de entorno recomendadas:
OLLAMA_NUM_GPU=1
OLLAMA_GPU_OVERHEAD=0
OLLAMA_MAX_LOADED_MODELS=1
OLLAMA_NUM_PARALLEL=1
CUDA_VISIBLE_DEVICES=0
OLLAMA_KEEP_ALIVE=30m        # Mantener modelo en memoria 30 min
OLLAMA_NUM_THREAD=8          # Threads CPU (tu i9 tiene 24, usa 8)
```

---

## üöÄ Resultado Final

Con GPU optimizada + timeouts corregidos:

- **Primera consulta:** 15-30 segundos (carga modelo)
- **Siguientes consultas:** 5-15 segundos ‚ö°‚ö°‚ö°
- **Uso GPU:** 80-90%
- **Uso RAM:** ~22GB (de tus 64GB)

---

## üìù Notas

1. **Primera consulta siempre m√°s lenta:** Ollama carga el modelo (20GB) en GPU
2. **Mantener modelo "caliente":** Configurar `OLLAMA_KEEP_ALIVE=30m`
3. **Si falla GPU:** Ollama autom√°ticamente vuelve a CPU (m√°s lento pero funcional)
4. **RTX 4070 perfecta:** 12GB VRAM es suficiente para deepseek-coder:33b

---

## ‚úÖ Checklist

- [ ] Configurar variables de entorno
- [ ] Verificar `nvidia-smi` funciona
- [ ] Reiniciar Ollama completamente
- [ ] Ejecutar `ollama run deepseek-coder:33b`
- [ ] Verificar `ollama ps` muestra 80-90% GPU
- [ ] Probar consulta en tu app
- [ ] Medir tiempo (deber√≠a ser ~10-20s)
